{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx:dict=None, add_unk:bool=True, unk_token:str=\"<UNK>\"):\n",
    "        \"\"\"Create vocabulary and index dictionaries.\"\"\"\n",
    "        self._token_to_idx = {} if (token_to_idx is None) else token_to_idx\n",
    "        self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
    "    \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = self.add_token(unk_token) if add_unk else -1\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\"Return serializable dictionary.\"\"\"\n",
    "        return {\"token_to_idx\": self._token_to_idx, \"add_unk\": self._add_unk, \"unk_token\": self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents:dict):\n",
    "        \"\"\"Instantiate Vocabulary from serialized dict.\"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token:str):\n",
    "        \"\"\"Add word and return its index.\"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "    \n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens:List[str]):\n",
    "        \"\"\"Add words and get their indexes.\"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token:str):\n",
    "        \"\"\"Return the index or UNK index for a given word.\"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            idx = self._token_to_idx.get(token, self.unk_index)\n",
    "        else: \n",
    "            idx = self._token_to_idx[token]\n",
    "            \n",
    "        return idx\n",
    "    \n",
    "    def lookup_index(self, index:int):\n",
    "        \"\"\"Return the word for a given index.\"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"The index {index} is not in the Vocabulary.\")\n",
    "            \n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    def __init__(self, review_vocabulary:Vocabulary, rating_vocabulary:Vocabulary):\n",
    "        \"\"\"Create a vectorizer to one-hot encode words in reviews.\"\"\"\n",
    "        self.review_vocabulary = review_vocabulary\n",
    "        self.rating_vocabulary = rating_vocabulary\n",
    "        \n",
    "    def vectorize(self, review:str):\n",
    "        \"\"\"Create one-hot encodings of words in review.\"\"\"\n",
    "        one_hot_encoding = np.zeros(len(self.review_vocabulary), dtype=np.float32)\n",
    "        \n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot_encoding[self.review_vocabulary.lookup_token(token)] = 1\n",
    "                \n",
    "        return one_hot_encoding\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, dataset:pd.DataFrame, word_freq_filter:int=25):  \n",
    "        \"\"\"Instantiate vectorizer from dataset.\"\"\"\n",
    "        review_vocabulary = Vocabulary(add_unk=True)\n",
    "        rating_vocabulary = Vocabulary(add_unk=False)\n",
    "        \n",
    "        for rating in sorted(set(dataset.rating)):\n",
    "            rating_vocabulary.add_token(rating)\n",
    "            \n",
    "        word_counter = Counter()\n",
    "        for review in dataset.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counter[word] += 1\n",
    "                    \n",
    "        for word, count in word_counter.items():\n",
    "            if count > word_freq_filter:\n",
    "                review_vocabulary.add_token(word)\n",
    "                \n",
    "        return cls(review_vocabulary, rating_vocabulary)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents:dict):\n",
    "        \"\"\"Instantiate vectorizer from serializable object.\"\"\"\n",
    "        review_vocabulary = Vocabulary.from_serializable(contents[\"review_vocabulary\"])\n",
    "        rating_vocabulary = Vocabulary.from_serializable(contents[\"rating_vocabulary\"])\n",
    "        \n",
    "        return cls(review_vocabulary, rating_vocabulary)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\"Create serializable object.\"\"\"\n",
    "        return {\"review_vocabulary\":self.review_vocabulary.to_serializable(), \n",
    "                \"rating_vocabulary\":self.rating_vocabulary.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, dataset:pd.DataFrame, vectorizer:ReviewVectorizer):\n",
    "        \"\"\"Create lookup dictionary containing Tr-Val-Te datasets, and store the vocabulary vectorizer.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        train = dataset[dataset.split==\"train\"]\n",
    "        validation = dataset[dataset.split==\"val\"]\n",
    "        test = dataset[dataset.split==\"test\"]\n",
    "        \n",
    "        self._lookup_dict = {\n",
    "            \"train\": (train, len(train)),\n",
    "            \"validation\": (validation, len(validation)),\n",
    "            \"test\": (test, len(test))\n",
    "        }\n",
    "        \n",
    "        self.set_split(\"train\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, dataset_path:str):\n",
    "        \"\"\"Load dataset from file, and create vectorizer.\"\"\"\n",
    "        dataset = pd.read_csv(dataset_path)\n",
    "        train_dataset = dataset[dataset.split == \"train\"]\n",
    "        return cls(dataset, ReviewVectorizer.from_dataframe(train_dataset))\n",
    "    \n",
    "    def load_dataset_and_vectorizer(cls, dataset_path:str, vectorizer_path:str):\n",
    "        \"\"\"Load dataset and vectorizer from files.\"\"\"\n",
    "        dataset = pd.read_csv(dataset_path)\n",
    "        vectorizer = self.load_vectorizer(vectorizer_path)\n",
    "        return cls(dataset, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer(vectorizer_path:str):\n",
    "        \"\"\"Load vectorizer from file.\"\"\"\n",
    "        with open(vectorizer_path) as f:\n",
    "            return ReviewVectorizer.from_serializable(json.load(f))\n",
    "    \n",
    "    def save_vectorizer(self, vectorizer_path:str):\n",
    "        \"\"\"Save serializable vectorizer.\"\"\"\n",
    "        with open(vectorizer_path, \"w\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"Return vocabulary vectorizer.\"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split:str=\"train\"):\n",
    "        \"\"\"Set dataset split (train, validation or test).\"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_data, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        \"\"\"Get features and label for a review at index.\"\"\"\n",
    "        row = self._target_data.iloc[index]\n",
    "        vector_review = self._vectorizer.vectorize(row.review)\n",
    "        index_rating = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        return {\"x\":vector_review, \"y\":index_rating}\n",
    "    \n",
    "    def get_number_batches(self, batch_size:int):\n",
    "        \"\"\"Return number of batches according to length of the lookup dictionary split.\"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset:pd.DataFrame, batch_size:int, shuffle:bool=True, drop_last:bool=True, device:str=\"cpu\"):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, num_features:int):\n",
    "        \"\"\"Instantiate classifier, and its layers.\"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor, apply_sigmoid:bool=False):\n",
    "        \"\"\"Define forward pass.\"\"\"\n",
    "        y_out = self.fc1(x).squeeze()\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t../output/vectorizer.json\n",
      "\t../output/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "def make_train_state(args:dict):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'validation_loss': [],\n",
    "            'validation_acc': [],\n",
    "            'test_loss': [],\n",
    "            'test_acc': [],\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args:dict, model:ReviewClassifier, train_state:dict):\n",
    "    if train_state['epoch_index'] == 0:     # Save one model\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "    elif train_state['epoch_index'] >= 1:   # Save model if improved.\n",
    "        loss_t = train_state['val_loss'][-1:]\n",
    "\n",
    "        if loss_t >= train_state['early_stopping_best_val']: # Loss is worse\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        else: \n",
    "            torch.save(model.state_dict(), train_state['model_filename'])\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred:torch.Tensor, y_target:torch.Tensor):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def set_seed_everywhere(seed:int, cuda:bool):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath:str):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='../../Chapter3/data/preprocess_data.csv',\n",
    "    save_dir='../output/',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    print(\"Loading dataset and vectorizer\")\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv,\n",
    "                                                            args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    # create dataset and vectorizer\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c30666043347f286e49da7a7f09aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', style=ProgressStyle(description_width='initial')),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d62a05cdc6f47dab2739a51b0406fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Train', max=1, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2315ed0251d042db864b1c238b431e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Validation', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Train setup ##\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc=\"Training\", total=args.num_epochs, position=0)\n",
    "\n",
    "dataset.set_split(\"train\")\n",
    "train_bar = tqdm_notebook(desc=\"Train\", total=dataset.get_number_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "dataset.set_split(\"validation\")\n",
    "val_bar = tqdm_notebook(desc=\"Validation\", total=dataset.get_number_batches(args.batch_size), position=1, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(dataset, args, split, bar=None):\n",
    "    dataset.set_split(split)\n",
    "    batch_generator = generate_batches(dataset=dataset, batch_size=args.batch_size, device=args.device)\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "\n",
    "    if split == \"train\":\n",
    "        classifier.train()\n",
    "    else:\n",
    "        classifier.eval()\n",
    "\n",
    "    for step, batch in enumerate(batch_generator):\n",
    "        if split == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        y_pred = classifier(x=batch[\"x\"].float())\n",
    "\n",
    "        loss = loss_fn(y_pred, batch[\"y\"].float())\n",
    "        epoch_loss += (loss.item() - epoch_loss) / (step+1)\n",
    "\n",
    "        if split == \"train\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        accuracy = compute_accuracy(y_pred, batch[\"y\"].float())\n",
    "        epoch_accuracy += (accuracy - epoch_accuracy) / (step+1)\n",
    "\n",
    "        if bar is not None:\n",
    "            bar.set_postfix(loss=epoch_loss, acc=epoch_accuracy, epoch=epoch)\n",
    "            bar.update()\n",
    "\n",
    "    train_state[split+\"_loss\"].append(epoch_loss)\n",
    "    train_state[split+\"_acc\"].append(epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "        step(dataset=dataset, args=args, split=\"train\", bar=train_bar)\n",
    "        step(dataset=dataset, args=args, split=\"validation\", bar=val_bar)\n",
    "        \n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "        \n",
    "        scheduler.step(train_state[\"validation_loss\"][-1])\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "        if train_state[\"stop_early\"]:\n",
    "            break\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0\n",
      "Test accuracy: 0.0\n",
      "this is a pretty awesome book -> negative\n"
     ]
    }
   ],
   "source": [
    "#classifier.load_state_dict(torch.load(train_state[\"model_filename\"]))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "step(dataset=dataset, args=args, split=\"test\", bar=None)\n",
    "\n",
    "print(f\"Test loss: {train_state['test_loss'][-1]}\")\n",
    "print(f\"Test accuracy: {train_state['test_acc'][-1]}\")\n",
    "\n",
    "test_review = \"this is a pretty awesome book\"\n",
    "      \n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(review=test_review, classifier=classifier, vectorizer=vectorizer, class_threshold=0.5)\n",
    "\n",
    "print(f\"{test_review} -> {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text:str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review:str, classifier:ReviewClassifier, vectorizer:ReviewVectorizer, class_threshold:float=0.5):\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    \n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    probability = torch.sigmoid(result).item()\n",
    "    index = 0 if probability < class_threshold else 1\n",
    "    \n",
    "    return vectorizer.rating_vocabulary.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion = Enum(\"opinion\", \"positive, negative\")\n",
    "\n",
    "def list_influential_words_for_class(op, vectorizer, num_of_words=20, word_weights=None):\n",
    "    if op == opinion.positive:\n",
    "        intro = \"Most\"\n",
    "        _, indices = torch.sort(word_weights, dim=0, descending=True)\n",
    "    else:\n",
    "        intro = \"Least\"\n",
    "        _, indices = torch.sort(word_weights, dim=0, descending=False)\n",
    "    \n",
    "    indices = indices.numpy().tolist()\n",
    "    \n",
    "    print(f\"{intro} influential words for {op.name} Reviews:\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    for i in range(num_of_words):\n",
    "        print(vectorizer.review_vocabulary.lookup_index(indices[i]))\n",
    "    print(\"----------\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most influential words for positive Reviews:\n",
      "----------------------------------------------\n",
      "go\n",
      "from\n",
      "and\n",
      "not\n",
      "food\n",
      "us\n",
      "this\n",
      "or\n",
      "that\n",
      "are\n",
      "back\n",
      "you\n",
      "be\n",
      "was\n",
      "it\n",
      "service\n",
      "what\n",
      "t\n",
      "when\n",
      "so\n",
      "----------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights_of_words = classifier.fc1.weight.detach()[0]\n",
    "list_influential_words_for_class(opinion.positive, vectorizer, num_of_words=20, word_weights=weights_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least influential words for negative Reviews:\n",
      "----------------------------------------------\n",
      "just\n",
      "would\n",
      "a\n",
      "very\n",
      "good\n",
      "she\n",
      "my\n",
      "<UNK>\n",
      "as\n",
      "no\n",
      "were\n",
      "our\n",
      "n\n",
      "all\n",
      "had\n",
      "is\n",
      "only\n",
      "can\n",
      "to\n",
      "there\n",
      "----------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_influential_words_for_class(opinion.negative, vectorizer, num_of_words=20, word_weights=weights_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "        \n",
    "        dataset.set_split(\"train\")\n",
    "        batch_generator = generate_batches(dataset=dataset, batch_size=args.batch_size, device=args.device)\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "        \n",
    "        classifier.train()\n",
    "        \n",
    "        for step, batch in enumerate(batch_generator):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = classifier(x=batch[\"x\"].float())\n",
    "            \n",
    "            loss = loss_fn(y_pred, batch[\"y\"].float())\n",
    "            epoch_loss += (loss.item() - epoch_loss) / (step+1)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            accuracy = compute_accuracy(y_pred, batch[\"y\"].float())\n",
    "            epoch_accuracy += (accuracy - epoch_accuracy) / (step+1)\n",
    "            \n",
    "            train_bar.set_postfix(loss=epoch_loss, acc=epoch_accuracy, epoch=epoch)\n",
    "            train_bar.update()\n",
    "        \n",
    "        train_state[\"train_loss\"].append(epoch_loss)\n",
    "        train_state[\"train_acc\"].append(epoch_accuracy)\n",
    "        \n",
    "        \n",
    "        dataset.set_split(\"validation\")\n",
    "        batch_generator = generate_batches(dataset=dataset, batch_size=args.batch_size, device=args.device)\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "        \n",
    "        classifier.eval()\n",
    "        \n",
    "        for step, batch in enumerate(batch_generator):\n",
    "            y_pred = classifier(x=batch[\"x\"].float())\n",
    "            \n",
    "            loss = loss_fn(y_pred, batch[\"y\"].float())\n",
    "            epoch_loss += (loss.item() - epoch_loss) / (step+1)\n",
    "        \n",
    "            accuracy = compute_accuracy(y_pred, batch[\"y\"].float())\n",
    "            epoch_accuracy += (accuracy - epoch_accuracy) / (step+1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=epoch_loss, acc=epoch_accuracy, epoch=epoch)\n",
    "            val_bar.update()\n",
    "        \n",
    "        train_state[\"validation_loss\"].append(epoch_loss)\n",
    "        train_state[\"validation_acc\"].append(epoch_accuracy)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "        \n",
    "        scheduler.step(train_state[\"validation_loss\"][-1])\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "        if train_state[\"stop_early\"]:\n",
    "            break\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
